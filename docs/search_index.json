[["index.html", "STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 8 1.1 Implementing neural networks in R", " STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 8 In week 8, we have studied neural networks, which consists an input layer, an output layer and any number of hidden layers specified by the user. In this lab, we will see how to fit neural networks in R and Python. 1.1 Implementing neural networks in R To fit a relatively simple neural network (by simple, it means the network does not contain too many hidden layers, is not optimised using too advanced algorithms, and/or does not include regularisation techniques such as dropout), we use the neuralnet function from the neuralnet package. In particular, we need to specify the number of hidden layers and hidden nodes using the argument hidden, the loss function using err.fct, the activation function using act.fct, the activation function for the output layer using linear.output (set to TRUE for a linear activation function and FALSE to be the same as the activation function specified in act.fct). There are many other arguments could be manually set, such as the optimisation algorithm and learning rate; see help page of neuralnet for more details. library(neuralnet) # an example neural network for regression tasks Model &lt;- neuralnet(Y~X1+X2+..., data, hidden=c(5), err.fct=&quot;sse&quot;, act.fct=&quot;logistic&quot;, linear.output=TRUE) # The network contains a single hidden layers with 5 hidden nodes. # The network uses sum of squared errors as the loss function. # The network uses logistic function as the activation function for hidden nodes # and linear function as the activation function for output nodes. # an example neural network for classification tasks softplus &lt;- function(x) log(1+exp(x)) Model &lt;- neuralnet(Y~X1+X2+..., data, hidden=c(5,3), err.fct=&quot;ce&quot;, act.fct=softplus, linear.output=FALSE) # The network contains two hidden layers with 5 and 3 hidden nodes in each layer. # The network uses cross entropy as the loss function. # The network uses a user-defined softplus function as the activation function for hidden nodes and output nodes. # The softplus function is a smooth approximation to rectified linear unit (ReLU) function. To visualise the neural network, we could use the default plot function or plotnet function from the NeuralNetTools package. # option 1 plot(Model) # option 2 # library(devtools); install_github(&#39;fawda123/NeuralNetTools&#39;) library(NeuralNetTools) plotnet(Model) To use the model for predicting new observations, we could use the default predict function or the compute function. This returns the probability of the observation in each class and hence requires further conversion to evaluate the classification performance. # option 1 test_predict_prob&lt;- predict(model, test_data) # option 2 test_predict_prob&lt;- neuralnet::compute(model, test_data)$net.result # convert to class label for binary classification test_predict_class &lt;- ifelse(test_predict_prob&gt;0.5, class1, class2) #class 1 and class 2 are the class names # convert to class label for multi-class classification test_predict_class &lt;- apply(test_predict_prob, 1, which.max) "],["exercise-1-task-2-in-lecture-notes.html", "2 Exercise 1: Task 2 in lecture notes", " 2 Exercise 1: Task 2 in lecture notes QUESTION: On the Boston data, fit five neural networks such that the input layer has four nodes (crime.rate, low.socio.status, aver.rooms and river.bounds) the output layer has one node (median.value) one hidden layer with the logistic function as its activation function and different number of hidden nodes, specifically from 1 up to 5. For each neural network (i.e. the network with the specific number of hidden nodes), estimate the sum of squared errors (SSEs) for the training and test data sets. Visualise the SSEs with a graph and comment on its output. The following codes were included in the lecture note to pre-process the data and split the data into training and test sets. # load the data library(MASS) data(Boston) Boston &lt;- Boston[,c(&quot;medv&quot;,&quot;crim&quot;,&quot;lstat&quot;,&quot;rm&quot;,&quot;rad&quot;,&quot;chas&quot;)] colnames(Boston) &lt;- c(&quot;median.value&quot;,&quot;crime.rate&quot;,&quot;low.socio.status&quot;, &quot;aver.rooms&quot;,&quot;index.radial.highways&quot;,&quot;river.bounds&quot;) # min-max normalisation maxs &lt;- apply(Boston, 2, max) mins &lt;- apply(Boston, 2, min) scaled &lt;- as.data.frame(scale(Boston, center = mins, scale = maxs - mins)) # train-test random splitting set.seed(84) index &lt;- sample(1:nrow(Boston),round(0.75*nrow(Boston))) train_Boston&lt;- scaled[ index,] test_Boston &lt;- scaled[-index,] Hint: Write down the R command to fit a neural network on this dataset. Hint Suppose the hidden layer has only one hidden node. The corresponding R code is as follows. Model &lt;- neuralnet(median.value~crime.rate+low.socio.status+aver.rooms+river.bounds, data=train_Boston, hidden=c(1), linear.output=TRUE) Write down the R commands to calculate SSEs for the training and test data sets. Hint Slightly different from the usual formula for sum of squared errors, we divide it by two so that the formula is consistent with the neuralnet package: \\[\\frac{\\sum(\\text{observed}-\\text{fitted values})^2}{2}\\] # training SSEs train_SSE &lt;- sum((Model$net.result[[1]]-train_Boston$median.value)^2)/2 test_pred &lt;- neuralnet::compute(Model,test_Boston[,c(&quot;crime.rate&quot;, &quot;low.socio.status&quot;,&quot;aver.rooms&quot;)]) test_SSE &lt;- sum((test_pred$net.result[[1]]-test_Boston$median.value)^2)/2 Combining the two hints to solve the question. Solution library(neuralnet) set.seed(84) # create an empty dataframe to save training and test SSE. SSE &lt;- data.frame(matrix(nrow=5, ncol=3)) colnames(SSE) &lt;- c(&quot;no.hidden nodes&quot;,&quot;train SSE&quot;,&quot;test SSE&quot;) # train the neural network and record training and test SSE for (i in 1:5){ SSE[i,1] &lt;- i #record the number of hidden nodes nn_boston &lt;- neuralnet(median.value~crime.rate+low.socio.status+aver.rooms+river.bounds, data=train_Boston, hidden=c(i), linear.output=TRUE) #fit the model SSE[i,2] &lt;- sum((nn_boston$net.result[[1]] - train_Boston[,&quot;median.value&quot;])^2)/2 #record training SSE test_pred &lt;- neuralnet::compute(nn_boston, test_Boston[, c(&quot;crime.rate&quot;,&quot;low.socio.status&quot;,&quot;aver.rooms&quot;,&quot;river.bounds&quot;)]) SSE[i,3] &lt;- sum((test_pred$net.result[[1]] - test_Boston[, &quot;median.value&quot;])^2)/2 #record test SSE } SSE ## no.hidden nodes train SSE test SSE ## 1 1 2.128904 4.342257 ## 2 2 1.686398 3.460076 ## 3 3 1.488804 3.271197 ## 4 4 1.208661 3.345709 ## 5 5 1.201828 3.091838 To visualise the SSEs, we create a bar plot using ggplot. # Bar plot of results library(ggplot2);library(tibble) Regression_NN_Errors &lt;- tibble(Network = rep(c(&quot;NN1&quot;, &quot;NN2&quot;, &quot;NN3&quot;, &quot;NN4&quot;, &quot;NN5&quot;), each = 2), DataSet = rep(c(&quot;Train&quot;, &quot;Test&quot;), time = 5), SSE = c(t(SSE[,2:3]))) nn_ggplot &lt;- Regression_NN_Errors %&gt;% ggplot(aes(Network, SSE, fill = DataSet)) + geom_col(position = &quot;dodge&quot;) + ggtitle(&quot;Neural networks SSE (different number of nodes in the hidden layer)&quot;) nn_ggplot We can notice that: the training SSE is decreasing while the number of hidden nodes increases, and the test SSE is not monotonically decreasing and the lowest test SSE is achieved when the network has five nodes in the hidden layer. The first finding should not be surprising as adding more hidden nodes increases model capacity and hence the training error will decrease. However, a larger model may suffer from overfitting and hence the test error may not always decreasing. In general, we prefer the \"least\" complex neural network with the low(est) SSE on the test data set. Note that using different criteria could give us totally different results. In addition, be cautious that the current result is produced based on a single training of neural network. Since the optimisation of neural network is sensitive to initialisation, running the algorithm again is likely to lead to different conclusions. To alleviate such randomness, consider adding the argument rep into neuralnet; e.g. including rep=10 will force the same network to be trained for ten times. "],["exercise-2-german-data.html", "3 Exercise 2: German data", " 3 Exercise 2: German data This exercise extends Examples 6-8 in lecture note. Task Fit multiple neural networks with Account_balance, Purpose, Length_of_cur_employment and Credit_Amount as predictors in your model. Hide Remember to pre-process the data for both categorical variables and continuous variables before fitting the neural network. The categorical variables should be transformed into dummy variables using one-hot encoding and the continuous variables should be scaled using either standardisation or min-max normalisation. Compare the cross-entropy loss, Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) of different models, and select the optimal model. Hide To get AIC and BIC, set the argument likelihood=TRUE in neuralnet. For the optimal model you selected in part (2), report its test performance. Solution load(&quot;German.RData&quot;) train &lt;- German_train # Data pre-processing min_max_scale &lt;- function(x){ (x - min(x)) / (max(x) - min(x)) } train$Credit_amount &lt;- min_max_scale(train$Credit_amount) credit_card_matrix &lt;- model.matrix(~Account_balance+Purpose+Length_of_cur_employment +Credit_amount, data=train) credit_card_matrix_final &lt;- credit_card_matrix[,-1] Next we fit a few neural networks with different width (i.e. the number of hidden nodes) and depth (i.e. the number of hidden layers). train$Creditability &lt;- as.integer(train$Creditability)-1 predictor_list &lt;- paste(colnames(credit_card_matrix_final),collapse=&quot;+&quot;) f &lt;- paste(c(&quot;train$Creditability~&quot;,predictor_list),collapse=&quot;&quot;) set.seed(84) nn_credit_one_layer &lt;- neuralnet(f,data=credit_card_matrix_final,hidden=c(5), linear.output = FALSE,err.fct = &#39;ce&#39;, likelihood=TRUE, threshold = 0.1) nn_credit_two_layers_1 &lt;- neuralnet(f,data=credit_card_matrix_final, hidden=c(4,1), linear.output = FALSE,err.fct = &#39;ce&#39;, likelihood=TRUE, threshold = 0.1) nn_credit_two_layers_2 &lt;- neuralnet(f,data=credit_card_matrix_final, hidden=c(1,4), linear.output = FALSE,err.fct = &#39;ce&#39;, likelihood=TRUE, threshold = 0.1) nn_credit_two_layers_3 &lt;- neuralnet(f,data=credit_card_matrix_final, hidden=c(5,3), linear.output = FALSE,err.fct = &#39;ce&#39;, likelihood=TRUE, threshold = 0.1) Now we produce a bar plot comparing all built models. library(ggplot2); library(dplyr) Class_NN_ICs &lt;- tibble(&#39;Network&#39; = rep(c(&quot;NN_1L&quot;,&quot;NN_2L_1&quot;, &quot;NN_2L_2&quot;, &quot;NN_2L_3&quot;), each = 3), &#39;Metric&#39; = rep(c(&#39;AIC&#39;, &#39;BIC&#39;,&#39;CE loss&#39;), length.out=12), &#39;Value&#39; = c(nn_credit_one_layer$result.matrix[4,1], nn_credit_one_layer$result.matrix[5,1], nn_credit_one_layer$result.matrix[1,1], nn_credit_two_layers_1$result.matrix[4,1], nn_credit_two_layers_1$result.matrix[5,1], nn_credit_two_layers_1$result.matrix[1,1], nn_credit_two_layers_2$result.matrix[4,1], nn_credit_two_layers_2$result.matrix[5,1], nn_credit_two_layers_2$result.matrix[1,1], nn_credit_two_layers_3$result.matrix[4,1], nn_credit_two_layers_3$result.matrix[5,1], nn_credit_two_layers_3$result.matrix[1,1])) nn_ggplot &lt;- Class_NN_ICs %&gt;% ggplot(aes(Network, Value, fill=Metric)) + geom_col(position = &#39;dodge&#39;) + ggtitle(&quot;AIC, BIC, and cross entropy loss of the neural networks&quot;) nn_ggplot First recall that a smaller value of AIC and BIC indicates a better model. In terms of AIC the fourth neural network with 2 hidden layer and 5 and 3 nodes in each layer seemed to be best, while BIC preferred the second neural network with 2 hidden layers and 4 and 1 nodes in each layer. The cross entropy loss agreed with AIC so we would probably choose the fourth neural network. This is an example of one of the many times where information criteria give different answer since they penalise complexity in different ways. We can also double check the previous comments, and find out which neural network is the one with the smallest value for the cross-entropy loss function, by using the which.min command. which.min(c(nn_credit_one_layer$result.matrix[4,1], nn_credit_two_layers_1$result.matrix[4,1], nn_credit_two_layers_2$result.matrix[4,1], nn_credit_two_layers_3$result.matrix[4,1])) ## aic ## 4 which.min(c(nn_credit_one_layer$result.matrix[5,1], nn_credit_two_layers_1$result.matrix[5,1], nn_credit_two_layers_2$result.matrix[5,1], nn_credit_two_layers_3$result.matrix[5,1])) ## bic ## 3 which.min(c(nn_credit_one_layer$result.matrix[1,1], nn_credit_two_layers_1$result.matrix[1,1], nn_credit_two_layers_2$result.matrix[1,1], nn_credit_two_layers_3$result.matrix[1,1])) ## error ## 4 To predict on the test data, we need to first clean the data as for the training data. # Data pre-processing test &lt;- German_test test$Credit_amount &lt;- min_max_scale(test$Credit_amount) test$Creditability &lt;- as.integer(test$Creditability)-1 test_credit_card_matrix &lt;- model.matrix(~Account_balance+Purpose+Length_of_cur_employment +Credit_amount, data=test) test_credit_card_matrix_final &lt;- test_credit_card_matrix[,-1] # Prediction test_pred &lt;- predict(nn_credit_two_layers_3,test_credit_card_matrix_final) table(test$Creditability,test_pred&gt;0.5) ## ## FALSE TRUE ## 0 60 97 ## 1 78 265 "],["exercise-3-dividend-data.html", "4 Exercise 3: Dividend data", " 4 Exercise 3: Dividend data In this example, our goal is to develop a neural network to determine if a stock pays a dividend or not. The dataset is stored under dividendinfo.csv, which includes one response variable and five predictor variables. dividend (class): A value of 1 indicates that the stock pays a dividend; 0 indicates that the stock that does not pay a dividend. fcfps: Free cash flow per share (in $) earnings_growth: Earnings growth in the past year (in %) de: Debt to Equity ratio mcap: Market Capitalization of the stock current_ratio: Current Ratio (or Current Assets/Current Liabilities) Task Read in the data and perform exploratory analysis. What have you observed? Solution dividend &lt;- read.csv(&quot;dividendinfo.csv&quot;) # some example codes for numerical summaries summary(dividend) library(skimr) skim(dividend) # some example codes for graphical summaries pairs(dividend) libray(GGally) ggpairs(dividend) par(mfrow=c(3,2)); invisible(lapply(2:ncol(dividend),function(i) boxplot(dividend[,i]~dividend$dividend))) Pre-process and split the data to prepare for training and evaluating a neural network. Hint As all variables are continuous and they have quite different ranges, scale them either using standardisation or min-max normalisation. Solution While there are built-in functions such as scale to standardise the entire data, the best practice is to split the data into training and test first and then apply standardisation/normalisation. This could avoid information leakage from training to test data. # Data split set.seed(1) idx &lt;- sample(nrow(dividend),0.8*nrow(dividend)) train &lt;- dividend[idx,] test &lt;- dividend[-idx,] # option 1: Standardise the data means &lt;- apply(train[,2:6], 2, mean) sds &lt;- apply(train[,2:6], 2, sd) train.std &lt;- scale(train[,2:6]) train.std &lt;- cbind(train[,1],train.std) test.std &lt;- scale(test[,2:6], means, sds) test.std &lt;- cbind(test[,1],test.std) # option 2: Normalise the data min_max_scale_test &lt;- function(x_tr, x_te){ mins &lt;- apply(x_tr, 2, min) maxs &lt;- apply(x_tr, 2, max) x_te &lt;- rbind(mins,maxs, x_te) x_te &lt;- apply(x_te, 2, function(x) (x-x[1])/(x[2]-x[1])) x_te &lt;- x_te[-c(1:2),] } train.norm &lt;-apply(train[,2:6], 2, function(x) (x-min(x))/(max(x)-min(x)) ) train.norm &lt;-cbind(train[,1],train.norm) test.norm &lt;-min_max_scale_test(train[,2:6], test[,2:6]) test.norm &lt;-cbind(test[,1], test.norm) Build a neural network with a single hidden layer, any number of hidden nodes, and the logistic function as the activation function. Interpret the relative importance of variables using the garson function. Solution set.seed(1) nn_di &lt;- neuralnet(dividend~., data=train, hidden=c(5), err.fct=&quot;ce&quot;, act.fct=&quot;logistic&quot;, linear.output=FALSE, likelihood=TRUE) garson(nn_di) We can see that the variable current_ratio is the one with the strongest relationship with the response variable dividend, followed by de, fcfps and mcap. The variable earnings_growth has the least relationship with dividend. Fit the above model multiple times using the argument rep and select the optimal model. Solution set.seed(1) nn_di &lt;- neuralnet(dividend~., data=train, hidden=c(5), err.fct=&quot;ce&quot;, act.fct=&quot;logistic&quot;, linear.output=FALSE, likelihood=TRUE, rep=5) # plot(nn_di) plot(nn_di, rep=&quot;best&quot;) From the plots (plot(nn_di)), we can see that the optimisation algorithm stops at different iterations (from Steps) and lead to differnt cross-entropy loss (from Error). In general, training the network longer decreases the cross-entropy loss. However, this decrease takes place on the training data set and may not generalise to the test. In other words, training the model longer may increase the risk of overfitting. To select the optimal model, we could look at the AIC and BIC values. which.min(nn_di$result.matrix[4,]) #AIC ## [1] 3 which.min(nn_di$result.matrix[5,]) #AIC ## [1] 3 AIC and BIC agree in this case and they both choose the third repetition. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
