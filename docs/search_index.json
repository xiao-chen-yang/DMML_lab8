[["index.html", "STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 8", " STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 8 In week 8, we learned about performing clustering by using hierarchical agglomerative clustering (HAC), visualising HAC via dendrograms, and selecting the optimal number of clusters according to the silhouette width. The command for hierarchical agglomerative clustering is hclust. Setting the method argument selects the linkage method. It takes as its first argument a distance matrix. The distance matrix can be created using the command dist (default method is Euclidean distance). d &lt;- dist(data,method=&quot;euclidian&quot;) hiclust &lt;- hclust(d, method=&quot;single&quot;) To plot the resulting dendrogram, the plot command is used on the hclust fitted object. If the dendrogram suggests \\(g\\) clusters, we could determine the cluster assignment by using the command cutree within the dendextend package with argument k set to \\(g\\). Alternatively, if the dendrogram should be cut at height \\(l\\), we could set the argument h to \\(l\\). The result will be a vector with length equal to the number of observations, each entry identifying which cluster the corresponding object is assigned to. plot(hiclust) library(dendextend) clusters &lt;- cutree(hiclust, k=3) # cut the dendrogram to form 3 clusters clusters &lt;- cutree(hiclust, h=20) # cut the dendrogram at the height of 20 It is sometimes also helpful to visualise the data giving different colours to observations belonging to different clusters. This can be done by using the function color_branches within the dendextend package. library(dendextend) dend &lt;- as.dendrogram(hiclust,hang=0) dend_cut_3clusters &lt;- color_branches(dend,k=3) #3 clusters plot(dend_cut_3clusters, horiz=TRUE) To determine the optimal number of clusters and/or assess the performance of clustering algorithms, silhouette plots can be used. The silhouette value ranges from −1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. In R, we could either use the function silhouette after clustering. d &lt;- dist(data,method=&quot;euclidian&quot;) hiclust &lt;- hclust(d, method=&quot;single&quot;) clusters &lt;- cutree(hiclust, k=2) plot(silhouette(clusters), dist(data), col=1:2) The function fviz_nbclust from the package factoextra can produce a plot of the average silhouette width against the number of clusters, which helps determine the optimal number of clusters. We would have to use the FUN=hcut argument to specify hierarchical clustering. library(factoextra) ggplot_fviz &lt;-fviz_nbclust(USArrests,FUN=hcut,method = &quot;silhouette&quot;) ggplot_fviz "],["exercise-1-tasks-2-7-in-lecture-notes.html", "2 Exercise 1: Tasks 2-7 in lecture notes", " 2 Exercise 1: Tasks 2-7 in lecture notes In Example 3 of Week 8 lecture note, we have performed HAC on USArrests data using the following commands. USArrests &lt;- scale(USArrests) usarrests.clus &lt;- hclust(dist(USArrests)) plot(as.dendrogram(usarrests.clus,hang = 0),ylab=&quot;complete linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) Figure 2.1: Dendrogram resulting from HAC on USArrests dataset Note that in the above R code, we start by scaling/standardising the data using the function scale. This is important as we don’t want the clustering algorithm to depend to an arbitrary variable unit. Reading the dendrogram from the right to the left, we see that New Hampshire and Iowa seem to be the first two leaves that constitute a clade. Therefore, we can conclude that New Hampshire and Iowa are most similar to each other. TASK 2: Let’s say that we now want to focus on the cluster containing the states New Hampshire, Iowa and Maine. Which cluster is the most similar to that one? Solution It's the cluster containing Nebraska, Montana and Idaho. It would be wrong if your answer was the singleton state of Idaho. Because that (singleton) cluster is first merged together with the cluster of Nebraska and Montana, and the new cluster (comprised of Nebraska, Montana and Idaho) is merged together with the cluster of New Hampshire, Iowa and Maine. In Example 4, we allocate the observations to three clusters by using cutree. library(dendextend) dend_usarrests &lt;- as.dendrogram(usarrests.clus,hang=0) allocations &lt;- cutree(dend_usarrests,k=3) head(allocations) ## Alabama Alaska Arizona Arkansas California Colorado ## 1 1 2 3 2 2 TASK 3: Instead of reading from the dendrogram, can you find a way to check that New Hampshire and Iowa are the first two leaves that constitute a clade? Hint: Have a look at usarrests.clus$merge and ?hclust. Solution The merge component of the hclust fitted object gives us an \\(n-1\\) by 2 matrix. Let's have a look at it: head(usarrests.clus$merge) ## [,1] [,2] ## [1,] -15 -29 ## [2,] -13 -32 ## [3,] -14 -16 ## [4,] -23 -49 ## [5,] -36 3 ## [6,] -20 -31 Row \\(i\\) of merge describes the merging of clusters at step \\(i\\) of the clustering. If an element \\(j\\) in the row is negative, then observation \\(j\\) was merged at this stage. If \\(j\\) is positive then the merge was with the cluster formed at the (earlier) stage \\(j\\) of the algorithm. Thus negative entries in merge indicate agglomerations of observations (i.e. singleton clusters), and positive entries indicate agglomerations of non-singletons clusters. Looking at the first row, we can see that the \\(15\\)th and \\(29\\)th observations were merged together. These are Iowa and New Hampshire respectively. rownames(USArrests)[c(15,29)] ## [1] &quot;Iowa&quot; &quot;New Hampshire&quot; Note that the 5th row of the merge matrix has a positive number 3 in it. This means that observation 36 (indicated by a -36) is joined to the cluster created in row 3, i.e. observations 14 and 16. So the row creates a cluster of three observations: 36, 14 and 16. TASK 4: How could we get the names for the members of the third cluster? Hint Names of US states are given as row names of the dataset, which can be found by using row.names(data). Observations that belong to the third cluster can be found by using the condition [allocation==3]. Solution One way to do it is the following: rownames(USArrests)[allocations==3] ## [1] &quot;Arkansas&quot; &quot;Connecticut&quot; &quot;Delaware&quot; &quot;Hawaii&quot; ## [5] &quot;Idaho&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; ## [9] &quot;Kentucky&quot; &quot;Maine&quot; &quot;Massachusetts&quot; &quot;Minnesota&quot; ## [13] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;New Hampshire&quot; ## [17] &quot;New Jersey&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; ## [21] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Dakota&quot; ## [25] &quot;Utah&quot; &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; ## [29] &quot;West Virginia&quot; &quot;Wisconsin&quot; &quot;Wyoming&quot; TASK 5: Based on Figure 2.1, determine how many clusters you would obtain if you cut the dendrogram at the heights 2.9 and 2. If the dendrogram is cut at height 2.9, clusters will be obtained. If the dendrogram is cut at height 2, clusters will be obtained. Solution In the first case (i.e. cutting the dendrogram at 2.9) we would end up with 6 clusters while in the second case (i.e. cutting the dendrogram at 2) we would end up with 11 clusters. This is a result of Alaska being a singleton cluster that is merged after those heights. It can be verified in R either quantitatively or visually by using the commands below. library(dendextend) usarrests.clus &lt;- hclust(dist(USArrests)) dend_usarrests &lt;- as.dendrogram(usarrests.clus) # cut at height 2.9 table(cutree(dend_usarrests,h=2.9)) ## ## 1 2 3 4 5 6 ## 7 1 11 7 14 10 dendrogram_cut_height &lt;- color_branches(dend_usarrests,h=2.9) plot(dendrogram_cut_height,ylab=&quot;complete linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) abline(v=2.9,lty=2,lwd=2) # cut at height 2 table(cutree(dend_usarrests,h=2)) ## ## 1 2 3 4 5 6 7 8 9 10 11 ## 4 1 8 4 3 10 4 6 3 3 4 dendrogram_cut_2height &lt;- color_branches(dend_usarrests,h=2) plot(dendrogram_cut_2height,ylab=&quot;complete linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) abline(v=2,lty=2,lwd=2) TASK 6: Assume that you have information that Florida and Illinois should be clustered together and at the same time that Florida and Colorado should be in different clusters. What can you do to make this happen? Solution Looking at the dendrogram, you would have to cut the dendrogram before the height where Florida and Colorado merge (e.g. \\(2.3\\)) and after the height where Florida and Illinois merge (e.g. \\(1.8\\)). The previous tasks all focus on the HAC result using complete linkage as the linkage criterion. Let's now consider other linkage criterion and see it changes the clustering. TASK 7 Perform HAC using the following three linkage criteria - single, average, and centroid. Produce the dendrograms and comment on the plots. Use the cuttree command to split the observations into 4 clusters. Plot the results using the pairs function and comment on them. Hide The linkage criterion is defined by the method argument within the hclust function. Look at Example 6 in the lecture note for using pairs. Solution usarrests.clus.single &lt;- hclust(dist(USArrests),method = &quot;single&quot;) usarrests.clus.average &lt;- hclust(dist(USArrests),method = &quot;average&quot;) usarrests.clus.centroid &lt;- hclust(dist(USArrests),method = &quot;centroid&quot;) For completeness, the dendrogram of HAC with complete linkage is also plotted. plot(as.dendrogram(usarrests.clus,hang = 0),ylab=&quot;complete linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) plot(as.dendrogram(usarrests.clus.single,hang = 0),ylab=&quot;single linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) plot(as.dendrogram(usarrests.clus.average,hang = 0),ylab=&quot;average linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) plot(as.dendrogram(usarrests.clus.centroid,hang = 0),ylab=&quot;centroid linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) Comparing the dendrograms produced from HAC with complete linkage and single linkage, it is clear that single linkage shows the chaining pattern, where clusters were merged together due to single elements being close to each other. The average linkage balances out between complete linkage and single linkage. The centroid linkage shows the issue of inversion, e.g. between Delaware and Arkansas. four.clus.complete &lt;- cutree(usarrests.clus,k=4) pairs(USArrests,col=four.clus.complete,lower.panel=NULL,main=&quot;HAC with complete linkage&quot;) four.clus.single &lt;- cutree(usarrests.clus.single,k=4) pairs(USArrests,col=four.clus.single,lower.panel=NULL,main=&quot;HAC with single linkage&quot;) four.clus.average &lt;- cutree(usarrests.clus.average,k=4) pairs(USArrests,col=four.clus.average,lower.panel=NULL,main=&quot;HAC with average linkage&quot;) four.clus.centroid &lt;- cutree(usarrests.clus.centroid,k=4) pairs(USArrests,col=four.clus.centroid,lower.panel=NULL,main=&quot;HAC with centroid linkage&quot;) There is a clear pattern that HAC with single linkage clusters almost all observations into one group, compared with cluster linkage leads to roughly equal number of observations in each cluster. Centroid linkage also results in the vast majority of observations in a single cluster. While different linkage criteria lead to different clustering results as shown in the pairs plot, without any prior knowledge about the ground truth label of the observation, we cannot conclude which one is better. All we could do is to comment on the clustering performance, e.g. based on the silhouette width. QUESTION: Select the optimal number of clusters for HAC with complete, single, average and centroid linkage according to the average silhouette width. After that, suggest which HAC result produces the best clustering for this dataset and comment on the performance. "],["exercise-2-freedman-data.html", "3 Exercise 2: Freedman data", " 3 Exercise 2: Freedman data In this exercise, we will look at a data set called \"Freedman\" from the car package. The \"Freedman\" data frame has 110 rows and 4 columns. The observations are U. S. metropolitan areas with 1968 populations of 250,000 or more. library(car) data &lt;- Freedman QUESTION: Perform exploratory data analysis and if necessary, clean the data to prepare for hierarchical agglomerative clustering. Hint Check the variable type, which could inform which distance metrics to be used. Solution A quick way to check the data is by using the skim function: library(skimr) skim(data) Table 3.1: Data summary Name data Number of rows 110 Number of columns 4 _______________________ Column type frequency: numeric 4 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist population 10 0.91 1135.99 1560.14 270.0 398.75 664.0 1167.75 11551.0 ▇▁▁▁▁ nonwhite 0 1.00 10.80 10.26 0.3 3.40 7.2 14.88 64.3 ▇▂▁▁▁ density 10 0.91 765.67 1441.95 37.0 266.50 412.0 773.25 13087.0 ▇▁▁▁▁ crime 0 1.00 2714.08 991.40 458.0 2066.75 2698.0 3305.00 5441.0 ▂▆▇▃▁ From this output, we notice that this data set has some missing values. The simplest way to handle missing values is by removing all rows containing missing value: data &lt;- na.omit(data) There are more sophisticated methods for imputing missing values. However, we'll skip this as it is beyond the scope of the exercise. In addition, we notice that the four variables in this data set have very different mean values and standard deviations. Therefore, a scaling operation should be performed. data &lt;- scale(data) Perform HAC using three different linkage criteria. Produce the dendrograms and comment on the plots. Solution Below is a sample code; you should modify the arguments to change the linkage criteria. # Dissimilarity matrix d &lt;- dist(data, method = &quot;euclidean&quot;) # Hierarchical clustering using Complete Linkage hc1 &lt;- hclust(d, method = &quot;complete&quot; ) # Plot the obtained dendrogram plot(hc1, hang=0, cex=0.5) To generate more beautiful dendrogram visualisations, check this article. Use the silhouette width to determine the optimal number of clusters. Solution We could either produce the silhouette plots for different number of clusters and then compare the silhoette width, or use the fviz_nbclust function from the factoextra package which generates a plot of silhouette width against the number of clusters. First, let's try produce the silhouette plots for 2-5 clusters. library(cluster); library(dendextend) dend1 &lt;- as.dendrogram(hc1, hang=0) # Cut the dendrogram to create 2/3/4/5 clusters allocations_2clusters &lt;- cutree(dend1,k=2) allocations_3clusters &lt;- cutree(dend1,k=3) allocations_4clusters &lt;- cutree(dend1,k=4) allocations_5clusters &lt;- cutree(dend1,k=5) # Plot the obtained dendrogram plot(silhouette(allocations_2clusters,dist(data)), col = c(&quot;black&quot;, &quot;red&quot;), main=&quot;Silhouette plot (2 clusters)&quot;) plot(silhouette(allocations_3clusters,dist(data)), col = c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;), main=&quot;Silhouette plot (3 clusters)&quot;) plot(silhouette(allocations_4clusters,dist(data)), col = c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;), main=&quot;Silhouette plot (4 clusters)&quot;) plot(silhouette(allocations_5clusters,dist(data)), col = c(&quot;black&quot;, &quot;red&quot;, &quot;green&quot;, &quot;blue&quot;,&quot;magenta&quot;), main=&quot;Silhouette plot (5 clusters)&quot;) One thing we notice from the four plots is that the first cluster contains a large number of samples while the other clusters have very little samples. When splitting the dataset into more clusters, the newly added clusters have only 1 observations. Therefore, it seems sufficient to consider only two clusters. Let's now try the fviz_nbclust function: library(factoextra) ggplot_fviz &lt;-fviz_nbclust(data, FUN=hcut, method=&quot;silhouette&quot;, hc_method=&quot;complete&quot;) ggplot_fviz Similar to our previous finding, using two clusters gives the highest average silhouette width. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
