[["index.html", "STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 8", " STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 8 In week 8, we learned about performing clustering by using hierarchical agglomerative clustering (HAC), visualising HAC via dendrograms, and selecting the optimal number of clusters according to the silhouette width. The command for hierarchical agglomerative clustering is hclust. Setting the method argument selects the linkage method. It takes as its first argument a distance matrix. The distance matrix can be created using the command dist (default method is Euclidean distance). d &lt;- dist(data,method=&quot;euclidian&quot;) hiclust &lt;- hclust(d, method=&quot;single&quot;) To plot the resulting dendrogram, the plot command is used on the hclust fitted object. If the dendrogram suggests \\(g\\) clusters, we could determine the cluster assignment by using the command cutree within the dendextend package with argument k set to g. Alternatively, if the dendrogram should be cut at height \\(l\\), we could set the argument h to l. The result will be a vector with length equal to the number of observations, each entry identifying which cluster the corresponding object is assigned to. plot(hiclust) library(dendextend) clusters &lt;- cutree(hiclust, k=3) clusters &lt;- cutree(hiclust, h=20) It is sometimes also helpful to visualise the data giving different colours to observations belonging to different clusters. This can be done by using the function color_branches within the dendextend package. library(dendextend) dend &lt;- as.dendrogram(hiclust,hang=0) dend_cut_3clusters &lt;- color_branches(dend,k=3) #3 clusters plot(dend_cut_3clusters, horiz=TRUE) To determine the optimal number of clusters and/or assess the performance of clustering algorithms, silhouette plots can be used. The silhouette value ranges from −1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighbouring clusters. In R, we could either use the function silhouette after clustering. d &lt;- dist(data,method=&quot;euclidian&quot;) hiclust &lt;- hclust(d, method=&quot;single&quot;) clusters &lt;- cutree(hiclust, k=2) plot(silhouette(clusters), dist(data), col=1:2) The function fviz_nbclust from the package factoextra can produce a plot of the average silhouette width against the number of clusters, which helps determine the optimal number of clusters. We would have to use the FUN=hcut argument to specify hierarchical clustering. library(factoextra) ggplot_fviz &lt;-fviz_nbclust(USArrests,FUN=hcut,method = &quot;silhouette&quot;) ggplot_fviz "],["exercise-1-tasks-2-7-in-lecture-notes.html", "2 Exercise 1: Tasks 2-7 in lecture notes", " 2 Exercise 1: Tasks 2-7 in lecture notes In Example 3 of Week 8 lecture note, we have performed HAC on USArrests data using the following commands. USArrests &lt;- scale(USArrests) usarrests.clus &lt;- hclust(dist(USArrests)) plot(as.dendrogram(usarrests.clus,hang = 0),ylab=&quot;complete linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) Figure 2.1: Dendrogram resulting from HAC on USArrests dataset Note that in the above R code, we start by scaling/standardising the data using the function scale. This is important as we don’t want the clustering algorithm to depend to an arbitrary variable unit. Reading the dendrogram from the right to the left, we see that New Hampshire and Iowa seem to be the first two leaves that constitute a clade. Therefore, we can conclude that New Hampshire and Iowa are most similar to each other. TASK 3: Instead of reading from the dendrogram, can you find a way to check that New Hampshire and Iowa are the first two leaves that constitute a clade? Hint: Have a look at usarrests.clus$merge and ?hclust. Solution The merge component of the hclust fitted object gives us an \\(n-1\\) by 2 matrix. Let's have a look at it: head(usarrests.clus$merge) ## [,1] [,2] ## [1,] -15 -29 ## [2,] -13 -32 ## [3,] -14 -16 ## [4,] -23 -49 ## [5,] -36 3 ## [6,] -20 -31 Row \\(i\\) of merge describes the merging of clusters at step \\(i\\) of the clustering. If an element \\(j\\) in the row is negative, then observation \\(j\\) was merged at this stage. If \\(j\\) is positive then the merge was with the cluster formed at the (earlier) stage \\(j\\) of the algorithm. Thus negative entries in merge indicate agglomerations of observations (i.e. singleton clusters), and positive entries indicate agglomerations of non-singletons clusters. Looking at the first row, we can see that the \\(15\\)th and \\(29\\)th observations were merged together. These are Iowa and New Hampshire respectively. rownames(USArrests)[c(15,29)] ## [1] &quot;Iowa&quot; &quot;New Hampshire&quot; Note that the 5th row of the merge matrix has a positive number 3 in it. This means that observation 36 (indicated by a -36) is joined to the cluster created in row 3, i.e. observations 14 and 16. So the row creates a cluster of three observations: 36, 14 and 16. TASK 2: Let’s say that we now want to focus on the cluster containing the states New Hampshire, Iowa and Maine. Which cluster is the most similar to that one? Solution It's the cluster containing Nebraska, Montana and Idaho. It would be wrong if your answer was the singleton state of Idaho. Because that (singleton) cluster is first merged together with the cluster of Nebraska and Montana, and the new cluster (comprised of Nebraska, Montana and Idaho) is merged together with the cluster of New Hampshire, Iowa and Maine. In Example 4, we allocate the observations to three clusters by using cutree. library(dendextend) dend_usarrests &lt;- as.dendrogram(usarrests.clus,hang=0) allocations &lt;- cutree(dend_usarrests,k=3) head(allocations) ## Alabama Alaska Arizona Arkansas California Colorado ## 1 1 2 3 2 2 TASK 4: How could we get the names for the members of the third cluster? Hint Names of US states are given as row names of the dataset, which can be found by using row.names(data). Observations that belong to the third cluster can be found by using the condition [allocation==3]. Solution One way to do it is the following: rownames(USArrests)[allocations==3] ## [1] &quot;Arkansas&quot; &quot;Connecticut&quot; &quot;Delaware&quot; &quot;Hawaii&quot; ## [5] &quot;Idaho&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; ## [9] &quot;Kentucky&quot; &quot;Maine&quot; &quot;Massachusetts&quot; &quot;Minnesota&quot; ## [13] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;New Hampshire&quot; ## [17] &quot;New Jersey&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; ## [21] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Dakota&quot; ## [25] &quot;Utah&quot; &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; ## [29] &quot;West Virginia&quot; &quot;Wisconsin&quot; &quot;Wyoming&quot; TASK 5: Based on Figure 2.1, determine how many clusters you would obtain if you cut the dendrogram at the heights 2.9 and 2. If the dendrogram is cut at height 2.9, clusters will be obtained. If the dendrogram is cut at height 2, clusters will be obtained. Solution In the first case (i.e. cutting the tree at 2.9) we would end up with 6 clusters while in the second case (i.e. cutting the tree at 2) we would end up with 11 clusters. This is a result of Alaska being a singleton cluster that is merged after those heights. It can be verified in R either quantitatively or visually by using the commands below. library(dendextend) usarrests.clus &lt;- hclust(dist(USArrests)) dend_usarrests &lt;- as.dendrogram(usarrests.clus) table(cutree(dend_usarrests,h=2)) dendrogram_cut_2height &lt;- color_branches(dend_usarrests,h=2) plot(dendrogram_cut_2height,ylab=&quot;complete linkage&quot;, xlab=&quot;Euclidean distance&quot;,horiz=TRUE) abline(v=2,lty=2,lwd=2) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
